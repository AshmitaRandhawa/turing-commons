# Science , Technology, and Society

In April 1945, Michael Polanyi—a chemist and sociologist of science—and Bertrand Russell—a philosopher and logician—were speaking on a radio programme about the practical implications of the famous formula, $E = mc^2$.

They were asked whether the formula had any practical applications for society, but neither could provide an answer.

Three months later the Manhattan project dropped the first of their three atomic bombs!

{cite:p}`bridgstock1998` draws attention to this story, because it was used originally by Polanyi, in his essay, 'The Republic of Science', to suggest that the practical and societal outcomes of pure scientific research were often unforeseen and unintended. The problem with this suggestion is that it implies that scientists cannot be held accountable or exercise any real responsibility for the consequences of their research—a troubling implication if true.

However, the definitions we have already encountered suggest that science, technology, and society are closely interconnected, and that RRI requires reflection upon the myriad ways that science and technology can impact and shape social norms and practices. Responsibility arises from this relationship, but if the impacts or consequences are unforeseeable and often unanticipated, then the principles of RRI may be too demanding.

Fortunately, the implications of Polanyi's example are narrow in scope. The following thought experiment will help us illustrate how.

Imagine a CEO of a large manufacturing company is approached by one of her scientific advisors and informed that a project that she has proposed will require an environmental impact assessment before it can proceed. The CEO dismisses this and orders that the project continue without the assessment. Furthermore, she callously proclaims that she does not care what the environmental consequences of the project may be. All she is interested in is making as much profit as possible. As it turns out, the project ends up causing vast amounts of pollution that cause irreparable harm to the nearby flora and fauna, and also affects the health of a community living downstream of the manufacturing plant. The CEO is, rightfully, held accountable, both morally and legally, and is prosecuted when her dismissal of the impact assessment is uncovered.

Few would take issue with the consequences for the CEO. She had a responsibility to ensure her company's processes operated in a safe and ethical manner, but chose to wilfully neglect this responsibility in spite of receiving advice from her staff.

Now, let's alter some of the details of the case but keep the logical structure the same.

This time, everything about the thought experiment remains the same except for the outcomes of the project. Instead, the project causes no harm. In fact, the efficiency of the new project actually reduces the company's emissions and leads to more sustainable operations.

There is no need to worry about accountability in this instance, as no harms occurred. But does the CEO deserve praise for her actions? This question is less likely to have a consensus among the answers.

Those that believe the CEO deserves no praise are likely to point to the fact that she did not carry out her due diligence or reflect upon the possible consequences of her project. She chose to ignore the suggestion of undertaking an environmental impact assessment, and, therefore, did not act in a responsible manner. She was unable to anticipate any harms or benefits because she did not gather the appropriate evidence. Neither did she act with deliberate intention, but instead acted in a careless manner. In short, she was lucky that the outcomes were positive.

Polanyi may be right to suggest that the societal impacts or consequences of some pure scientific research is hard to anticipate. This is especially true for the more distant effects. Consider again the long-term impact of the Human Genome Project, which is arguably still affecting current research. But this is not the case for all research or innovation projects.

There is no question about whether scientists, researchers, and developers have *some* responsibility for the applications of their research. This must be true for them to be *praised* for the positive outcomes, which they often are, and also to be held accountable and *blamed* for the negative consequences when they occur. The question is, rather, when they should receive praise and blame. While we will not try to answer this question, we will look at some of the practical ways that scientists, researchers, and developers can take responsibility for the social impacts of their research, in order to maximise the potential opportunities and minimise the possible harms associated with their work.

## Risk and Impact Assessments

At the [start of this chapter](responsibility.md) we looked at a definition of RRI from {cite:p}`vonschomberg2011`. Let's look at another one, this time from the European Commission:

```{tabbed} European Commission
> Responsible research and innovation is an approach that anticipates and assesses potential implications and societal expectations with regard to research and innovation, with the aim to foster the design of inclusive and sustainable research and innovation. {cite:p}`europeancommission2014`
```

```{tabbed} René von Schomberg
> “Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).” {cite:p}`vonschomberg2011`
```

There are many similarities between the two definitions, but they also emphasise different aspects through their choice of terminology. For instance, the definition from the European Commission refers to anticipation and assessment, Likely to emphasise the need for formal risk or impact assessment. There are many types of risk and impact assessments that can be carried out, such as safety and risk assessments, equality impact assessments, human rights impact assessments, and, of course, data protection impact assessments.

The necessity of such assessments will be familiar to those who work in commercial or public sector organisations, but less so to those in academic institutions. Typically they are carried out for compliance reasons. However the structured nature of such assessments can also support more ethical forms of reflection and anticipation.

It is not necessary to present an overview of all the different impact assessments that could be useful within the context of responsible research and innovation. However, the following resources are available for those who wish to explore some of them in more detail.

Instead, we will focus on two interwoven concepts that are central to almost all risk or impact assessments: identification of impacted stakeholders or users, and evaluation of the project's knowledge gaps (or, epistemic limitations).

## Inclusive and Deliberative Participation

:::{seealso} Why should stakeholders be included in a research or innovation project? :::



<!-- - The value of stakeholder engagement
  - Social value of ensuring that research and innovation are responsive to and supportive of social goals
  - Ethical value of mitigating bias or risk of harm that arises from limited perspectives
  - Instrumental value for effective research and innovation
- RRI relies on both the organic and creative nature of the human mind as much as it does the meticulous standards of the scientific method.
- The demands of responsibility require us to reflect on the systemic influences on the research and innovation lifecycle, and the wider impacts on society.
- RRI is about more than the avoidance of gross misconduct (e.g., plagiarism, fabrication/falsification of results, developing obviously dangerous technologies). Irresponsible research can often occur in the absence of any individual's malicious attitudes, but rather from a failure to consider how institutional practices influence the scientific process.
- Ulrich Beck, a German sociologist, calls this phenomenon “**organized irresponsibility**". -->

## Transparent and Accessible Project Documentation

One important component of the CEO's failure was her priority of a specific goal (i.e., profit) over potential competing goals (e.g., environmental sustainability).

